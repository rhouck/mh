{
 "metadata": {
  "name": "",
  "signature": "sha256:d8e8d3163ceac3d7e834bbbc332a6ec9671602903721b9d59ddc54537722f84c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# load source data\n",
      "# index records by auction_id\n",
      "data = pd.read_csv('source/20140129.0.click.0.csv', index_col=2)\n",
      "click = pd.DataFrame(data)\n",
      "\n",
      "data = pd.read_csv('source/20140129.0.view.0.csv', index_col=2)\n",
      "view = pd.DataFrame(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 452
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert unix timestamps to datetime\n",
      "for i in ['event_time','request_time', 'view_time']:\n",
      "    view[i] = pd.to_datetime(view[i]/1000000, unit='s')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 453
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add new column to view data signifiying whether an ad was clicked or not\n",
      "# initialize feature as zeros\n",
      "view['clicked'] = 0\n",
      "\n",
      "# select any click column and rename it to join with view dataframe\n",
      "click_series = click['event_type']\n",
      "click_series.name = 'event_type_click'\n",
      "\n",
      "# change clicked ad rows 'clicked' value to 1\n",
      "matched = view.join(click_series, how='inner')\n",
      "view.loc[matched.index, 'clicked'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 454
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# isolate columns that can be manipulated to affect CTR\n",
      "# ignore line item id\n",
      "# ignore url - assuming there are too many to make sense of, especially considering how few rows are available\n",
      "# ignore georgraphy\n",
      "# ignore time / day of week\n",
      "cols = ['clicked',\n",
      "        'creative_id', \n",
      "        'universal_site_id', \n",
      "        'adx_page_categories', \n",
      "        'matching_targeted_keywords', \n",
      "        'exchange', \n",
      "        'ad_position', \n",
      "        'matching_targeted_segments', \n",
      "        'device_type'\n",
      "        ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 455
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# drop universal site id when adx_page_categories is supplied"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 456
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test insertion order as only explanatory column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 457
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this function returns a sparce matrix containing hashed feature:value pairs for each feature vector (row)\n",
      "# with online (iterative) learning, it's impossible to know the full range of categorical values for a feature ahead of time\n",
      "# to get around this, we create new features for each unique feature:value pairs by storing hashed values to represent indices in a large vector\n",
      "\n",
      "from sklearn.feature_extraction import FeatureHasher\n",
      "\n",
      "# this is the hashing algorithm we'll be using to map the feature set of unknown complexity\n",
      "hasher = FeatureHasher(input_type='string', n_features=(2 ** 15))\n",
      "\n",
      "def build_row(raw_x):\n",
      "    x = []\n",
      "    for count, value in enumerate(raw_x):\n",
      "        if isinstance(value, str):    \n",
      "            vals = value.split()\n",
      "            for v in vals:\n",
      "                x.append('F%s:%s' % (count,v))\n",
      "            continue\n",
      "        if  np.isnan(value):\n",
      "            continue\n",
      "        if value:\n",
      "            x.append('F%s:%s' % (count,value))\n",
      "    \n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 458
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "# convert data set to bank array of sparce matrices\n",
      "for t, i in enumerate(view.index):\n",
      "        \n",
      "    # add new row to bank\n",
      "    y = view.ix[i][cols[0]]\n",
      "    raw_x = view.ix[i][cols[1:]]\n",
      "    \n",
      "    row = build_row(raw_x,)\n",
      "    \n",
      "    # create bank to store hashed rows\n",
      "    # store recent indexes to drop from data frame in batches\n",
      "    if t == 0:\n",
      "        rows = [row]\n",
      "        targets = [y]\n",
      "        ind_bank = [i]\n",
      "    else:\n",
      "        rows.append(row)\n",
      "        targets.append(y)\n",
      "        ind_bank.append(i)\n",
      "    \n",
      "    # monitor progress\n",
      "    if t % 20000 == 0 and t > 0:   \n",
      "        \n",
      "        # drop rows from data frame\n",
      "        view = view.drop(ind_bank)\n",
      "        ind_bank = []   \n",
      "        print '%s\\trows processed: %d' % (datetime.datetime.now(), t)\n",
      "\n",
      "view = view.drop(ind_bank)\n",
      "print \"Finished formatting rows for conversion to sparse matrices\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-12-07 22:19:09.869668\trows processed: 20000\n",
        "2014-12-07 22:19:28.635352\trows processed: 40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:19:43.619397\trows processed: 60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:19:58.343158\trows processed: 80000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:20:12.989960\trows processed: 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:20:27.559248\trows processed: 120000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:20:42.587144\trows processed: 140000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:20:57.468289\trows processed: 160000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:21:12.319822\trows processed: 180000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:21:26.934247\trows processed: 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:21:41.886060\trows processed: 220000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:22:00.442063\trows processed: 240000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:22:20.014201\trows processed: 260000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:22:35.690775\trows processed: 280000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:22:51.955119\trows processed: 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:23:06.771289\trows processed: 320000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:23:21.873544\trows processed: 340000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:23:38.695366\trows processed: 360000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:23:57.933029\trows processed: 380000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:24:28.208699\trows processed: 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:24:46.800037\trows processed: 420000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:25:05.116543\trows processed: 440000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:25:24.496529\trows processed: 460000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:25:43.070715\trows processed: 480000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:26:01.376004\trows processed: 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished formatting rows for conversion to sparse matrices"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 459
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import grid_search, cross_validation\n",
      "\n",
      "logr = LogisticRegression()\n",
      "parameters = {'C':[.001, .001, .01, .1, 1]}\n",
      "skf = cross_validation.StratifiedKFold(targets, n_folds=2,)\n",
      "clf = grid_search.GridSearchCV(logr, parameters, cv=skf)\n",
      "clf.fit(hashed_rows, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 470,
       "text": [
        "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[0 0 ..., 0 0], n_folds=2, shuffle=False, random_state=None),\n",
        "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'C': [0.001, 0.001, 0.01, 0.1, 1]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      }
     ],
     "prompt_number": 470
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "clf = SGDClassifier(loss='log', alpha=0.001)\n",
      "for i in range(len(targets)):\n",
      "    clf.partial_fit(hashed_rows[i], [targets[i]], np.asarray([0.,1.]))\n",
      "    # monitor progress\n",
      "    if i % 20000 == 0 and i > 0:   \n",
      "          print '%s\\trows processed: %d' % (datetime.datetime.now(), i)\n",
      "print \"Finished training all rows\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-12-07 22:38:53.124145\trows processed: 20000\n",
        "2014-12-07 22:38:59.778138\trows processed: 40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:06.467545\trows processed: 60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:13.193074\trows processed: 80000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:20.255118\trows processed: 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:27.456954\trows processed: 120000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:35.298239\trows processed: 140000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:42.656216\trows processed: 160000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:50.074793\trows processed: 180000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:39:57.795751\trows processed: 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:04.496504\trows processed: 220000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:11.186975\trows processed: 240000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:17.883430\trows processed: 260000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:24.784179\trows processed: 280000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:32.039901\trows processed: 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:39.338681\trows processed: 320000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:46.505622\trows processed: 340000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:40:53.392409\trows processed: 360000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:00.216381\trows processed: 380000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:06.939728\trows processed: 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:13.612375\trows processed: 420000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:20.530619\trows processed: 440000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:27.608455\trows processed: 460000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:34.512650\trows processed: 480000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-07 22:41:41.760638\trows processed: 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished training all rows"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 474
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}