{
 "metadata": {
  "name": "",
  "signature": "sha256:14d42ef668fedeca95a53b1a1aa54dcf01f6e8c6a0b588a1c58c099634803c19"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# load source data\n",
      "# index records by auction_id\n",
      "data = pd.read_csv('source/20140129.0.click.0.csv', index_col=2)\n",
      "click = pd.DataFrame(data)\n",
      "\n",
      "data = pd.read_csv('source/20140129.0.view.0.csv', index_col=2)\n",
      "view = pd.DataFrame(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert unix timestamps to datetime\n",
      "for i in ['event_time','request_time', 'view_time']:\n",
      "    view[i] = pd.to_datetime(view[i]/1000000, unit='s')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add new column to view data signifiying whether an ad was clicked or not\n",
      "# initialize feature as zeros\n",
      "view['clicked'] = 0\n",
      "\n",
      "# select any click column and rename it to join with view dataframe\n",
      "click_series = click['event_type']\n",
      "click_series.name = 'event_type_click'\n",
      "\n",
      "# change clicked ad rows 'clicked' value to 1\n",
      "matched = view.join(click_series, how='inner')\n",
      "view.loc[matched.index, 'clicked'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# isolate columns that can be manipulated to affect CTR\n",
      "# ignore line item id\n",
      "# ignore url - assuming there are too many to make sense of, especially considering how few rows are available\n",
      "# ignore georgraphy\n",
      "# ignore time / day of week\n",
      "cols = ['clicked',\n",
      "        'creative_id', \n",
      "        'universal_site_id', \n",
      "        'adx_page_categories', \n",
      "        'matching_targeted_keywords', \n",
      "        'exchange', \n",
      "        'ad_position', \n",
      "        'matching_targeted_segments', \n",
      "        'device_type'\n",
      "        ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# drop universal site id when adx_page_categories is supplied\n",
      "# test insertion order as only explanatory column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this function returns a sparce matrix containing hashed feature:value pairs for each feature vector (row)\n",
      "# with online (iterative) learning, it's impossible to know the full range of categorical values for a feature ahead of time\n",
      "# to get around this, we create new features for each unique feature:value pairs by storing hashed values to represent indices in a large vector\n",
      "\n",
      "from sklearn.feature_extraction import FeatureHasher\n",
      "\n",
      "# this is the hashing algorithm we'll be using to map the feature set of unknown complexity\n",
      "hasher = FeatureHasher(input_type='string', n_features=(2 ** 15))\n",
      "\n",
      "def build_row(raw_x):\n",
      "    x = []\n",
      "    for count, value in enumerate(raw_x):\n",
      "        if isinstance(value, str):    \n",
      "            vals = value.split()\n",
      "            for v in vals:\n",
      "                x.append('F%s:%s' % (count,v))\n",
      "            continue\n",
      "        if  np.isnan(value):\n",
      "            continue\n",
      "        if value:\n",
      "            x.append('F%s:%s' % (count,value))\n",
      "    \n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "# convert data set to bank array of sparce matrices\n",
      "for t, i in enumerate(view.index):\n",
      "        \n",
      "    # add new row to bank\n",
      "    y = view.ix[i][cols[0]]\n",
      "    raw_x = view.ix[i][cols[1:]]\n",
      "    \n",
      "    row = build_row(raw_x,)\n",
      "    \n",
      "    # create bank to store hashed rows\n",
      "    # store recent indexes to drop from data frame in batches\n",
      "    if t == 0:\n",
      "        rows = [row]\n",
      "        targets = [y]\n",
      "        ind_bank = [i]\n",
      "    else:\n",
      "        rows.append(row)\n",
      "        targets.append(y)\n",
      "        ind_bank.append(i)\n",
      "    \n",
      "    # monitor progress\n",
      "    if t % 20000 == 0 and t > 0:   \n",
      "        \n",
      "        # drop rows from data frame\n",
      "        view = view.drop(ind_bank)\n",
      "        ind_bank = []   \n",
      "        print '%s\\trows processed: %d' % (datetime.datetime.now(), t)\n",
      "\n",
      "view = view.drop(ind_bank)\n",
      "print \"Finished formatting rows for conversion to sparse matrices\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-12-08 11:16:40.905490\trows processed: 20000\n",
        "2014-12-08 11:16:58.274528\trows processed: 40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:15.433259\trows processed: 60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:31.513838\trows processed: 80000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:46.663034\trows processed: 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:02.246230\trows processed: 120000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:17.075161\trows processed: 140000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:32.536971\trows processed: 160000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:48.090804\trows processed: 180000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:02.823288\trows processed: 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:17.323143\trows processed: 220000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:32.018782\trows processed: 240000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:48.183427\trows processed: 260000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:04.210339\trows processed: 280000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:18.852487\trows processed: 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:34.881079\trows processed: 320000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:49.472519\trows processed: 340000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:03.858477\trows processed: 360000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:18.412324\trows processed: 380000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:32.531518\trows processed: 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:49.602597\trows processed: 420000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:04.324928\trows processed: 440000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:19.204008\trows processed: 460000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:34.089440\trows processed: 480000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:48.325447\trows processed: 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished formatting rows for conversion to sparse matrices"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hash rows for input into model\n",
      "hashed_rows = hasher.transform(rows)\n",
      "# convert targets to numpy array\n",
      "targets = np.asarray(targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split data into train / test sets\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "sss = StratifiedShuffleSplit(targets, n_iter=1)\n",
      "for train_index, test_index in sss:\n",
      "    train_index = train_index\n",
      "    test_index = test_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# confirm test / train / CV sets are appropriately balanced\n",
      "t = pd.Series(targets)\n",
      "print \"total: clicks - %s, views: %s\" % (t[t > 0].shape, t[t == 0].shape)\n",
      "print \"\"\n",
      "\n",
      "t = pd.Series(targets[test_index])\n",
      "print \"total - test set: clicks - %s, views: %s\" % (t[t > 0].shape, t[t == 0].shape)\n",
      "t = pd.Series(targets[train_index])\n",
      "print \"total - train set: clicks - %s, views: %s\" % (t[t > 0].shape, t[t == 0].shape)\n",
      "print \"\"\n",
      "\n",
      "ind = 1\n",
      "skf = cross_validation.StratifiedKFold(targets[train_index], n_folds=5,)\n",
      "for cv_train, cv_test in skf:\n",
      "    t = pd.Series(targets[cv_test])\n",
      "    print \"cv %s - test: clicks - %s, views: %s\" % (ind, t[t > 0].shape, t[t == 0].shape)\n",
      "    t = pd.Series(targets[cv_train])\n",
      "    print \"cv %s - train: clicks - %s, views: %s\" % (ind, t[t > 0].shape, t[t == 0].shape)\n",
      "    print \"\"\n",
      "    ind += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total: clicks - (155,), views: (518185,)\n",
        "\n",
        "total - test set: clicks - (15,), views: (51819,)\n",
        "total - train set: clicks - (140,), views: (466366,)\n",
        "\n",
        "cv 1 - test: clicks - (37,), views: (93265,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cv 1 - train: clicks - (100,), views: (373104,)\n",
        "\n",
        "cv 2 - test: clicks - (29,), views: (93272,)\n",
        "cv 2 - train: clicks - (108,), views: (373097,)\n",
        "\n",
        "cv 3 - test: clicks - (22,), views: (93279,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cv 3 - train: clicks - (115,), views: (373090,)\n",
        "\n",
        "cv 4 - test: clicks - (27,), views: (93274,)\n",
        "cv 4 - train: clicks - (110,), views: (373095,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "cv 5 - test: clicks - (22,), views: (93279,)\n",
        "cv 5 - train: clicks - (115,), views: (373090,)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import grid_search, cross_validation\n",
      "\n",
      "# split train data set into stratified train / test groups X times\n",
      "# each train / test group conains the full train data set\n",
      "# stratification aims to maintain the proportion of target values found in full train set in each CV train / test set\n",
      "logr = LogisticRegression()\n",
      "parameters = {'C':[.001, .001, .01, .1, 1]}\n",
      "skf = cross_validation.StratifiedKFold(targets[train_index], n_folds=5,)\n",
      "\n",
      "# high accuracy is proportion of correctly labeled events\n",
      "# high precision relates to a low false positive rate\n",
      "# high recall relates to a low false negative rate\n",
      "scoring = ('accuracy', 'precision', 'recall')\n",
      "for s in scoring:\n",
      "    clf = grid_search.GridSearchCV(logr, parameters, cv=skf, scoring=s)\n",
      "    clf.fit(hashed_rows[train_index], targets[train_index])\n",
      "    print \"Scoring metric: %s\" % (s)\n",
      "    for g in clf.grid_scores_:\n",
      "        print g\n",
      "    print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Scoring metric: accuracy\n",
        "mean: 0.99970, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.99970, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.99970, std: 0.00000, params: {'C': 0.01}\n",
        "mean: 0.99970, std: 0.00000, params: {'C': 0.1}\n",
        "mean: 0.99970, std: 0.00000, params: {'C': 1}\n",
        "\n",
        "Scoring metric: precision"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.01}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.1}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 1}\n",
        "\n",
        "Scoring metric: recall"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.001}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.01}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 0.1}\n",
        "mean: 0.00000, std: 0.00000, params: {'C': 1}\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}