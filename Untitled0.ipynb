{
 "metadata": {
  "name": "",
  "signature": "sha256:bd58369a97ed7b17db1b353e8460e0640d828f7e78e1bb269c0dff28a394e94f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# load source data\n",
      "# index records by auction_id\n",
      "data = pd.read_csv('source/20140129.0.click.0.csv', index_col=2)\n",
      "click = pd.DataFrame(data)\n",
      "\n",
      "data = pd.read_csv('source/20140129.0.view.0.csv', index_col=2)\n",
      "view = pd.DataFrame(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert unix timestamps to datetime\n",
      "for i in ['event_time','request_time', 'view_time']:\n",
      "    view[i] = pd.to_datetime(view[i]/1000000, unit='s')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add new column to view data signifiying whether an ad was clicked or not\n",
      "# initialize feature as zeros\n",
      "view['clicked'] = 0\n",
      "\n",
      "# select any click column and rename it to join with view dataframe\n",
      "click_series = click['event_type']\n",
      "click_series.name = 'event_type_click'\n",
      "\n",
      "# change clicked ad rows 'clicked' value to 1\n",
      "matched = view.join(click_series, how='inner')\n",
      "view.loc[matched.index, 'clicked'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# isolate columns that can be manipulated to affect CTR\n",
      "# ignore line item id\n",
      "# ignore url - assuming there are too many to make sense of, especially considering how few rows are available\n",
      "# ignore georgraphy\n",
      "# ignore time / day of week\n",
      "cols = ['clicked',\n",
      "        'creative_id', \n",
      "        'universal_site_id', \n",
      "        'adx_page_categories', \n",
      "        'matching_targeted_keywords', \n",
      "        'exchange', \n",
      "        'ad_position', \n",
      "        'matching_targeted_segments', \n",
      "        'device_type'\n",
      "        ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# drop universal site id when adx_page_categories is supplied\n",
      "# test insertion order as only explanatory column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this function returns a sparce matrix containing hashed feature:value pairs for each feature vector (row)\n",
      "# with online (iterative) learning, it's impossible to know the full range of categorical values for a feature ahead of time\n",
      "# to get around this, we create new features for each unique feature:value pairs by storing hashed values to represent indices in a large vector\n",
      "\n",
      "from sklearn.feature_extraction import FeatureHasher\n",
      "\n",
      "# this is the hashing algorithm we'll be using to map the feature set of unknown complexity\n",
      "hasher = FeatureHasher(input_type='string', n_features=(2 ** 15))\n",
      "\n",
      "def build_row(raw_x):\n",
      "    x = []\n",
      "    for count, value in enumerate(raw_x):\n",
      "        if isinstance(value, str):    \n",
      "            vals = value.split()\n",
      "            for v in vals:\n",
      "                x.append('F%s:%s' % (count,v))\n",
      "            continue\n",
      "        if  np.isnan(value):\n",
      "            continue\n",
      "        if value:\n",
      "            x.append('F%s:%s' % (count,value))\n",
      "    \n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "# convert data set to bank array of sparce matrices\n",
      "for t, i in enumerate(view.index):\n",
      "        \n",
      "    # add new row to bank\n",
      "    y = view.ix[i][cols[0]]\n",
      "    raw_x = view.ix[i][cols[1:]]\n",
      "    \n",
      "    row = build_row(raw_x,)\n",
      "    \n",
      "    # create bank to store hashed rows\n",
      "    # store recent indexes to drop from data frame in batches\n",
      "    if t == 0:\n",
      "        rows = [row]\n",
      "        targets = [y]\n",
      "        ind_bank = [i]\n",
      "    else:\n",
      "        rows.append(row)\n",
      "        targets.append(y)\n",
      "        ind_bank.append(i)\n",
      "    \n",
      "    # monitor progress\n",
      "    if t % 20000 == 0 and t > 0:   \n",
      "        \n",
      "        # drop rows from data frame\n",
      "        view = view.drop(ind_bank)\n",
      "        ind_bank = []   \n",
      "        print '%s\\trows processed: %d' % (datetime.datetime.now(), t)\n",
      "\n",
      "view = view.drop(ind_bank)\n",
      "print \"Finished formatting rows for conversion to sparse matrices\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-12-08 11:16:40.905490\trows processed: 20000\n",
        "2014-12-08 11:16:58.274528\trows processed: 40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:15.433259\trows processed: 60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:31.513838\trows processed: 80000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:17:46.663034\trows processed: 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:02.246230\trows processed: 120000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:17.075161\trows processed: 140000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:32.536971\trows processed: 160000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:18:48.090804\trows processed: 180000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:02.823288\trows processed: 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:17.323143\trows processed: 220000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:32.018782\trows processed: 240000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:19:48.183427\trows processed: 260000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:04.210339\trows processed: 280000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:18.852487\trows processed: 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:34.881079\trows processed: 320000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:20:49.472519\trows processed: 340000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:03.858477\trows processed: 360000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:18.412324\trows processed: 380000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:32.531518\trows processed: 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:21:49.602597\trows processed: 420000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:04.324928\trows processed: 440000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:19.204008\trows processed: 460000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:34.089440\trows processed: 480000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-12-08 11:22:48.325447\trows processed: 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished formatting rows for conversion to sparse matrices"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hash rows for input into model\n",
      "hashed_rows = hasher.transform(rows)\n",
      "# convert targets to numpy array\n",
      "targets = np.asarray(targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split data into train / test sets\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "sss = StratifiedShuffleSplit(targets, n_iter=1)\n",
      "for train_index, test_index in sss:\n",
      "    train_index = train_index\n",
      "    test_index = test_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import grid_search, cross_validation\n",
      "\n",
      "logr = LogisticRegression()\n",
      "parameters = {'C':[.001, .001, .01, .1, 1]}\n",
      "skf = cross_validation.StratifiedKFold(targets[train_index], n_folds=2,)\n",
      "clf = grid_search.GridSearchCV(logr, parameters, cv=skf)\n",
      "clf.fit(hashed_rows[train_index], targets[train_index])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[0 0 ..., 0 0], n_folds=2, shuffle=False, random_state=None),\n",
        "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'C': [0.001, 0.001, 0.01, 0.1, 1]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}